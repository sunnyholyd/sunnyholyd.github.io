---
title: Spark RDD 编程模型
description: 
    - 平时在工作中会经常使用 Spark 跑一些计算与统计的作业，虽然用的挺频繁，但是，感觉现在对 Spark 的了解却不是特别的全面，因为每次使用的时候都是针对当时的需求去解决问题，相当于每次都是学习一个特定的使用方式。这样碎片化的学习和了解，并不能使我对 Spark 有一个全面的了解。我想通过翻译 Spark 提供的官方英文文档，来对 Spark 有一个全面的学习和了解，翻译的过程中也会用现有的对 Spark 的理解来做一些改动和总结。
categories: Spark
tags: 
    - Spark
---


### 1.前言
Spark 提供的主要抽象是弹性分布式数据集（RDD），它是跨群集节点分区的元素集合，可以并行操作。 RDD 是通过从 Hadoop 文件系统（或任何其他 Hadoop 支持的文件系统）中的文件或 Driver 程序中的现有 Scala 集合开始并对其进行转换来创建的。用户还可以要求 Spark 在内存中保留 RDD，允许它在并行操作中有效地重用。最后，RDD 会自动从节点故障中恢复。

Spark中的第二个抽象是可以在并行操作中使用的共享变量。默认情况下，当Spark并行运行一个函数作为不同节点上的一组任务时，它会将函数中使用的每个变量的副本发送给每个任务。有时，变量需要跨任务共享，或者在任务和驱动程序之间共享。 Spark支持两种类型的共享变量：广播变量，可用于缓存所有节点的内存中的值;累加器，它们是仅做“累加”操作的变量，例如计数器和总和。

### 2.初始化Spark
构建一个Spark程序，必须要创建SparkContext对象，SparkContext对象会负责创建输入数据集。在创建SparkContext对象之前需要构建SparkConf对象，使用SparkConf对象来定义你的Spark程序的相关信息。
每个JVM只能有一个SparkContext被激活，在创建一个新的SparkContext之前，必须对旧的执行stop()操作。
相关代码如下：
```scala
val conf = new SparkConf().setAppName(appName).setMaster(master)
val sparkContext = new SparkContext(conf)
```
### 3.RDD的创建
RDD是弹性分布式数据集，作为Spark的核心要素。可以把RDD看作是支持并行化操作的容错集合，通常有两种方式去创建RDD，第一种是并行化处理一个已经存在的集合，第二种是引用一个外部存储系统的数据集，例如：共享文件系统，HDFS，HBase，以及任何支持Hadoop InputFormat的数据源。
#### 3.1.并行化集合
通过调用 SparkContext.parallelize 方法把现有的集合转化成一个 RDD，输入参数是一个已经创建好的列表集合，通过复制该集合元素去生成一个能被并行化操作的 RDD。代码示例如下所示：
```scala
val data = Array(1, 2, 3, 4, 5)
val distData = sc.parallelize(data)
```
并行化集合涉及到的一个重要的参数是 partitions，该参数需要传入一个整形数值，并行化集合会被切成paritions数量的份数。在Spark集群中每一个partition会分配一个task去处理。您需要为集群中的每个CPU分配2-4个paritions,通常，Spark会根据集群自动设置parition数。但是，您也可以通过将其作为第二个参数传递给parallelize（例如sc.parallelize（data，10））来手动设置它。注意：代码中的某些地方使用术语slices（分区的同义词）来保持向后兼容性。

#### 3.2.外部数据
可以从 Hadoop 支持的存储源中创建 RDD，存储源包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3等等。Spark 支持文本文件、SequenceFile(二进制文件)、以及 Hadoop InputFormat。
通过使用 textFile 方法创建文本文件的 RDD，是按行进行读取的，该方法需要传入文件的 URI 作为参数(例如，file://,hdfs://,s3a://等)。示例代码如下：
```scala
scala> val distFile = sc.textFile("data.txt")
distFile: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[10] at textFile at <console>:26
```
上面代码中通过读取外部的data.txt文本文件创建了名为distFile的RDD，创建的RDD就能用Spark提供的各种运算方法进行计算。
Spark中读取文件时需要注意：
+ 如果使用本地文件系统中的路径，那么该文件要保证在所有的worker节点中都是可以访问的。(需要把该文件复制到所有的worker节点中，或者使用共享文件系统)
+ Spark中所有基于文件的载入方法(例如textFile)，加载的路径都支持目录、压缩文件以及通配符的使用。例如，你可以使用textFile("/my/directory"),textFile("/my/directory/\*.txt"),以及 textFile("/my/directory/\*.gz").
+ textFile方法可以通过第二参数控制读取文件的partition数量。默认情况下，Spark会为每一个文件块(block)创建一个partition（HDFS的blocks默认是128MB），你可以把partition的数值设置的高一些，但是你却不能设置低于blocks数量的partitions数值。

除了textFile方法，Spark还提供了其他数据格式的方法：
1. SparkContext.wholeTextFiles 方法可以让你读取一个包含多个小文件的目录，读取返回的是 (filename,content) 的键值对。这里和 textFile 是不一样的，textFile 会返回文件中的每一行。分区由数据局部性决定，在某些情况下，可能导致分区太少。 对于这些情况，wholeTextFiles 提供了一个可选的第二个参数，用于控制最小数量的分区。
2. SparkContext.sequenceFile[K,V] 方法定义了 Key 和 Value 的类型参数。这两个类型都应该是 Hadoop 的 Writable 接口的子类，例如 IntWritable 和 Text 类。此外，Spark 允许您为一些常见的 Writable 指定本机类型;例如，sequenceFile[Int,String] 将自动读取 IntWritables 和 Texts。
3. 对于 Hadoop InputFormats 类型的输入，可以调用 SparkContext.hadoopRDD 方法，参数需要传入输入格式类、Key的类、以及Value的类。基于新的 MapReduce 的 API 则可以调用 SparkContext.newAPIHadoopRDD 方法。
4. RDD.saveAsObjectFile 和 SparkContext.objectFile 方法支持序列化 Java 对象的简单格式保存 RDD。虽然这不如 Avro 这样的专用格式有效，但它提供了一种保存任何 RDD 的简便方法。

### 4. RDD运算
RDD支持两种类型的运算：
1. 变换 transformation：对一个 RDD 进行变换操作后，会产生一个新的 RDD，新的 RDD 又可以进行新一轮的变换，采用了链式调用的设计模式。
2. 动作 action：RDD 在执行 action 操作后会返回具体的值或者持久化起来。
Transform 的处理是懒惰的,就是说 transform 的运算会先记录下来，等待执行到 action 操作的时候才会对 transform 操作进行实际的运算。这样的设计使得 Spark 具有更高的效率。 
默认情况下，每次你执行 action 操作的 RDD 上的transform 操作都会重新执行一遍。然而，你可以使用 persist(cache) 操作把 RDD 的数据持久化到内存或者磁盘中，下一次再使用这个 RDD 的时候就能省去再次计算的步骤，更加快速的获取 RDD 中的数据。

#### 4.1 基础示例
在这里简单介绍一个实例：
```scala
val lines = sc.textFile("data.txt")
val lineLengths = lines.map(s => s.length)
val totalLength = lineLengths.reduce((a, b) => a + b)
```
第一行通过一个外部文件创建了一个 RDD，这个 RDD 并没有加载到内存里面，也没有执行其他的操作，只是把lines指针指向了文件。第二行定义了 linLengths 作为 map 转换操作后的结果，同样的，因为懒惰的处理机制，map操作也没有立即进行计算。最后，我们调用了 reduce 操作，reduce 是一个 action 操作，此时，Spark 将计算分解为在不同机器上运行的任务，并且每台机器都运行其部分的 map 操作和本地的 reduce，然后返回结果。
如果我们想在后面继续使用 lineLengths，那么需要把该 RDD 进行持久化操作，可以加入如下代码：
```scala
lineLengths.persist()
```
这样就会使 lineLengths 在第一次计算之后就存到内存之中。

#### 4.2 将函数传递给Spark
Spark 的 API 很大程度上依赖于在驱动程序中传递函数以在集群上的运行，这里有两种推荐的方式：
1. 匿名函数
2. 静态函数

请注意，虽然也可以将引用传递给类实例中的方法（而不是单例对象），但这需要发送包含该类的对象以及方法。 例如：
```scala
class MyClass {
  def func1(s: String): String = { ... }
  def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(func1) }
}
```
在这里，如果我们创建一个新的 MyClass 实例并在其上调用 doStuff，那里的map会引用该 MyClass 实例的 func1 方法，因此需要将整个对象发送到集群。 它类似于编写rdd.map（x => this.func1（x））。
以类似的方式，访问外部对象的字段也将引用整个对象：
```scala
class MyClass {
  val field = "Hello"
  def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(x => field + x) }
}
```
相当于编写 rdd.map（x => this.field + x），它引用了所有这些。要避免此问题，最简单的方法是将字段复制到本地变量而不是从外部访问它：
```scala
def doStuff(rdd: RDD[String]): RDD[String] = {
  val field_ = this.field
  rdd.map(x => field_ + x)
}
```
**总结**
**尽量使用匿名函数、静态函数以及函数局部变量，以此来减少额外的对象成员在集群中传输带来的性能消耗。**

#### 4.3 理解闭包
当代码在 Spark 集群执行的时候，一个比较重要的事情是理解变量和方法的生命周期以及作用域。修改其范围之外的变量的 RDD 操作可能会经常引起混淆。在下面的示例中，我们将查看使用 foreach() 递增计数器的代码，但其他操作也可能出现类似问题。

**例子**
该例子是统计本地 RDD 元素的总数，根据是否在同一个 JVM 执行，它的表现可能不同。一个常见的例子是在本地模式下运行Spark（--master=local[n]）而不是将 Spark 应用程序部署到集群（例如通过spark-submit to YARN）：

```scala
var counter = 0
var rdd = sc.parallelize(data)

// Wrong: Don't do this!!
rdd.foreach(x => counter += x)

println("Counter value: " + counter)
```

**本地或集群模式**

上述代码的提交模式没有说明，可能无法按预期工作。为了执行作业，Spark 将 RDD 运算的处理分解为任务(Task)，每个任务由Executor 来执行。在执行之前，Spark会计算任务的闭包。闭包是那些必须在所有执行器中可见，并且需要在RDD上进行计算的变量和方法。闭包会被序列化，并发送给每个 Executor。

发送给每个 executor 的闭包内的变量是副本，因此，当在 foreach 函数中引用 counter 变量时，它不再是客户端 driver 程序节点上的 counter 变量。客户端驱动程序节点的内存中仍然有一个 counter，但执行器对其不可见！执行器只能看到序列化闭包中的副本。因此，counter 的最终值仍然为零，因为计数器上的所有操作都引用了序列化闭包内的值。
在本地模式下，在某些情况下，foreach 函数实际上将在与驱动程序相同的 JVM 中执行，并将引用相同的原始计数器，并且可能实际更新它。
为了确保在类似场景中的行为正常，应该使用累加器(Accumulator)。当执行环境分布的部署在集群中的工作节点中时，累加器提供了一种安全的更新变量的机制。
通常，闭包结构中像是循环以及本地定义的方法，不应该用于改变某些全局状态。Spark没有定义或保证从闭包外部引用的对象的突变行为。 执行此操作的某些代码可能在本地模式下工作，但这只是偶然的，并且此类代码在分布式模式下不会按预期运行。 如果需要某些全局聚合，请使用累加器。
Spark没有定义或保证从闭包外部引用的对象的改变行为。 执行此操作的某些代码可能在本地模式下工作，但这只是偶然的，并且此类代码在分布式模式下不会按预期运行。 如果需要某些全局聚合，请使用累加器。

**打印一个 RDD 中的元素**

另一个常见的习惯用法是尝试使用rdd.foreach(println)或rdd.map(println)打印出RDD的元素。在一台机器上，这将生成预期的输出并打印所有RDD的元素。但是，在集群模式下，执行器调用的stdout是在本地实现的，而不是驱动程序上的那个，因此驱动程序上的stdout不会显示这些！要打印驱动程序上的所有元素，可以使用collect()方法首先将RDD带到驱动程序节点：rdd.collect()。foreach(println)。 但是，这会导致驱动程序内存不足，因为collect()会将整个RDD提取到一台机器上; 如果你只需要打印RDD的一些元素，更安全的方法是使用take()：rdd.take(100).foreach(println)。

#### 4.4 使用键值对的 Pair RDD
尽管大多数的 Spark 运算都适用于任何类型的RDD，但一些特殊运算仅适用于键值对的Pair RDD。最常见的是分布式 “shuffle” 运算，例如通过 Key 对元素进行分组或聚合。 

在 Scala 中，这些运算在 Tuple2 类型（Scala 中的内置元组，通过简单编写（a，b）创建）的 RDD 上自动可用。 PairRDDFunctions 类中提供了键值对操作，它自动包装元组的 RDD。

例如，以下代码对 Pair RDD 使用 reduceByKey 操作来计算文件中每行文本出现的次数：
```scala
val lines = sc.textFile("data.txt")
val pairs = lines.map(s => (s, 1))
val counts = pairs.reduceByKey((a, b) => a + b)
```
例如，我们也可以使用 counts.sortByKey() 来按字母顺序对Key 进行排序，最后使用 counts.collect() 将它们作为对象数组返回到 driver 程序。

注意：在键值对操作中使用自定义对象作为键时，必须确保自定义 equals() 方法附带了匹配的 hashCode() 方法。有关完整详细信息，请参阅 Object.hashCode() 的方法文档描述。

#### 4.5 Transformation
下表展示了Spark中一些常用的变换操作。

名称 | 说明
--- | ---
map(func) | 返回一个新的RDD，新RDD的数据是通过源RDD的每一个元素执行func函数后生成的。
filter(func) | 源RDD中的每一个元素通过执行func函数，返回true的元素生成新的RDD。
flatMap(func) | 和map操作相似， 不过每一个源RDD中的元素经过func函数计算后，可以生成0到多个输出元素，func函数中应该返回一个顺序集合类型。
mapPartitions | 和map操作相似，但在RDD的每个分区（块）上单独运行，因此当在类型T的RDD上运行时，func必须是Iterator<T> => Iterator<U>类型。
mapPartitionsWithIndex(func) | 跟mapPartitions相似，但是func提供了表示分区索引的整数值，因此当在类型T的RDD上运行时，func必须是类型(Int，Iterator<T>)=>Iterator<U>。
sample(withReplacement, fraction, seed)	| 使用给定的随机数生成器种子，在有或没有替换(withReplacement)的情况下对数据进行一定比例(fraction)的采样。
union(otherDataset) | 把源RDD跟参数传入的RDD进行合并，生成一个新的RDD。原始RDD的partition的相对顺序不变。
intersection(otherDataset) | 返回包含源RDD和参数传入的RDD元素交集的新RDD，可以额外传入分区数和分区喊出的参数。
distinct([numPartitions]) | 内部先用map生成Pair RDD，然后在使用reduceByKey进行聚合，最后再用map生成非Pair的普通RDD。实现代码为：map(x => (x, null)).reduceByKey((x, y) => x, numPartitions).map(\_.\_1)
groupByKey([numPartitions]) | 需要使用PairRDD[K,V]进行调用，根据Key进行分组操作，会返回一个(K, Iterable<V>)参数类型的RDD。需要注意的是：1，如果你打算基于key进行分组聚合的操作（如求和、求平均值），使用reduceByKey和aggregateByKey操作会带来更高的性能。2，并发数是根据父RDD的分区数来的。
reduceByKey(func, [numPartitions]) | 需要使用PariRDD[K,V]进行调用，使用func函数把相同key中的value进行聚合操作，函数的类型必须为(V,V)=>V，reduce的Task数量可以通过第二个可选参数进行配置。
aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions]) | 需要使用PairRDD[K,V]进行调用，返回RDD[K,U]类型的数据集，该RDD中相同Key的Value通过使用combOp进行聚合，并且使用zeroValue作为初始值。这里允许聚合后的Value类型和输入的Value类型不同。
sortByKey([ascending], [numPartitions]) | 需要使用PairRDD[K,V]进行调用，根据Key进行生序或者降序排序，可以通过boolean类型的ascending参数进行设置。
join(otherDataset, [numPartitions]) | 可以使得PairRDD[K,V]和PairRDD[K,W]通过Key进行连接计算，返回的数据类型为RDD[K,(V,W)]，外连接支持leftOuterJoin,rightOuterJoin和fullOuterJoin.
cogroup(otherDataset, [numPartitions]) | 当在PairRDD[K,V]和PairRDD[K,W]上进行调用的时候，会返回PairRDD[K,(Iterable[V]),Iterable[W]]类型的数据集。
cartesian(otherDataset) | 笛卡尔积方法，RDD[T] 和 RDD[U] 经过该操作可以生成 RDD[T,U] 的结果。
pipe(command, [envVars]) | 通过 shell 命令传输 RDD 的每个分区，例如，Perl 或 bash 脚本。 RDD 元素被写入进程的 stdin，并且按行输出到 stdout。
coalesce(numPartitions) | 将RDD中的分区数减少为numPartitions。
repartition(numPartitions) | 随机 shuffle RDD 中的数据以创建更多或更少的分区并在它们之间进行平衡。这总是 shuffle 网络上的所有数据。
repartitionAndSortWithinPartitions(partitioner) | 根据给定的 partitioner 对 RDD 重新分区，并在每个生成的分区内按 Key 对记录进行排序。这比调用 reparition 然后再对每个分区进行排序更有效，因为它可以将排序推送到 shuffle 机制中。

#### 4.6 Actions
下面的表格列出了 Spark 支持的常用 Actions。详情请参考 RDD API 文档([Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)、[Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html)、[Python](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)、[R](http://spark.apache.org/docs/latest/api/R/index.html)) 以及 Pair RDD 函数文档([Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)、[Java](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html))。

Action | 说明
--- | ---
reduce(func) | 使用 func 函数（接受两个参数并返回一个）来 aggregate 数据集的元素。该函数应该满足交换律和结合律的，以便进行正确的并行计算。
collect() | 在 driver 中将 RDD 的所有元素作为数组返回。这通常在调用了 filter 或其他能够返回足够小的数据子集的运算之后使用。
count() | 返回数据集中的元素数。
first() | 返回数据集的第一个元素（类似于take（1））。
take(n) | 返回数据集的前n个元素的数组。
takeSample(withReplacement, num, [seed]) | 返回包含数据集的 num 个元素的随机样本的数组，withReplacement 参数指定是否有重复的元素，seed 参数指定随机数生成器种子。
takeOrdered(n, [ordering]) | 使用自然顺序或自定义的 ordering 返回RDD的前n个元素。
saveAsTextFile(path) | 将 RDD 中的元素作为文本文件（或文本文件集）写入本地文件系统、HDFS 或任何其他Hadoop 支持的文件系统的给定目录中。 Spark 将在每个元素上调用 toString，将其转换为文件中的一行文本。
saveAsSequenceFile(path) | 将 RDD 中的元素作为 Hadoop SequenceFile 写入本地文件系统、HDFS或任何其他 Hadoop 支持的文件系统中的给定路径中。这可以在实现 Hadoop 的 Writable 接口的键值对的 RDD 上使用。在 Scala 中，它也可以在可隐式转换为 Writable 的类型上使用（Spark 包括基本类型的转换，如 Int、Double、String 等）。
saveAsObjectFile(path) | 使用 Java 序列化以简单格式编写数据集的元素，然后可以使用 SparkContext.objectFile() 加载。
countByKey() | 仅适用于类型（K，V）的 RDD。 返回（K，Int）对的散列映射，其中包含每个键的计数。
foreach(func) | 在数据集的每个元素上运行函数 func。 这通常用于副作用，例如更新累加器或与外部存储系统交互。
注意：在 foreach() 之外修改除累加器之外的变量可能会导致未定义的行为。有关详细信息，请参阅了解闭包。

Spark RDD API 还公开了某些操作的异步版本，例如 foreach 的 foreachAsync，它会立即将一个 FutureAction 返回给调用者，而不是在完成操作时阻塞。 这可用于管理或等待操作的异步执行。

#### 4.7 Shuffle操作
在 Spark 中特定的运算会触发一个被称为 shuffle 的事件。Shuffle 是 Spark 重新分发数据的机制，以此可以对数据进行跨分区的分组操作。这通常涉及跨 Executors 和机器的复制数据，所以 shuffle 是复杂且昂贵的操作。

**背景**

为了了解在 shuffle 期间发生了什么，我们可以看一下 reduceByKey 操作的例子。reduceByKey 操作会生成一个新的 RDD，其中相同 key 的所有 value 会被合并成一个元组（key与该 key 关联的所有值执行 reduce 函数的结果），这里的挑战在于，并不是单个 key 对应的所有 value 都一定位于同一个的分区或机器上，但它们必须位于同一位置才能计算结果。

在 Spark 中，数据通常不跨分区分布，以便在必要的位置进行明确的操作。在计算期间，单个 task 将在单个分区上运行，因此，要组织所有数据的reduceByKey的reduce任务执行，Spark需要执行 all-to-all 操作。它必须读取所有的分区来发现所有的 key 对应的所有的 value，然后将分区中的值汇总在一起以计算每个 key 的最终结果-这称为shuffle。

尽管 shuffle 后的每个分区中的元素集将是确定性的，并且分区之间的排序也是确定的，但**分区内元素的排序并不是确定**的。如果在 shuffle 后需要可预测的有序数据，则可以使用：
+ mapPartitions 操作中使用排序操作（例如.sorted）对每个分区的元素集进行排序。
+ repartitionAndSortWithinPartitions 在同时重新分区的同时有效地在分区内进行排序。
+ sortBy 来创建一个全局有序的 RDD。

能够导致 shuffle 的运算包括 repartition 和 coalesce 等**重分区类操作**，groupByKey 和 reduceByKey 等**ByKey类操作**，以及 cogroup 和 join 等 **join 类操作**。

**性能影响**
Shuffle 是一个昂贵的操作，因为它涉及磁盘 I/O、数据序列化、网络I/O。为了组织 shuffle 的数据，Spark 会生成 map 任务去组织数据，并且生成一组 reduce 任务做聚合。这些术语来自 MapReduce，并不直接与 Spark 的 map 和 reduce 运算相关。

在内部，map 任务的结果会存储到内存中，直到内存被用尽。然后，这些数据会基于目标分区进行粗排序，并写入到单个文件中。最后，reduce 任务会读取对应的排序块。

某些 shuffle 操作会消耗大量的堆内存，因为在传输数据之前或之后，会使用内存中的数据结构来组织数据。具体来说， **reduceByKey 和 aggregateByKey 会在 map 侧创建这些结构，并且 'ByKey 操作会在 reduce 侧生成这些结构。**当内存被用尽时，Spark 会将这些数据溢出到磁盘，从而增加磁盘 I/O 和垃圾回收的额外开销。

Shuffle 还会在磁盘上生成大量中间文件。从 Spark 1.3 开始，这些文件将被保留，直到 RDD 不再使用，才会被垃圾回收。这样做是为了在重新计算 RDD 谱系时不需要重新创建 shuffle 文件。如果程序保留这些 RDD 的引用或 GC 不经常启动，则垃圾收集可能在很长一段时间后才会发生。这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间。配置 Spark 上下文时， spark.local.dir 配置参数可以指定临时存储目录。

可以通过调整各种配置参数来调节 Shuffle 行为。详情请参考 [Spark配置指南](http://spark.apache.org/docs/latest/configuration.html) 中的 'Shuffle行为' 部分。

**总结：** Spark 在 shuffle 时会生成 map 类任务和 reduce 类任务，map 任务会把数据加载到内存中。Shuffle 操作还会生成 RDD 相关的中间文件。

### 5. RDD 持久化
Spark 中最重要的功能之一是可以通过操作把数据集持久化（或缓存）到内存。当你持久化 RDD 时，每个结点都会存储内存中计算的任何分区，并在该数据集（或从中派生的数据集）的其他操作中重用它们。这使得将来的运算更快（通常超过10倍）。缓存是迭代算法和快速交互使用的关键工具。

你可以使用 persist() 或 cache() 方法标记要保留的RDD。 第一次在 action 中计算，就会保存在节点的内存中。Spark的缓存是容错的 - 如果丢失了RDD的任何分区，它将使用最初创建它的 transfromations 自动重新计算。

此外，每一个 RDD 都可以用不同的保存级别进行保存，从而允许你持久化数据集在硬盘，或者持久化在内存，甚至于跨结点复制。这些等级选择，是通过将一个 org.apache.spark.storage.StorageLevel 对象传递给persist()方法进行确定。cache()方法是使用默认存储级别的快捷方法，也就是 StorageLevel.MEMORY_ONLY(将反序列化的对象存入内存）。完整的存储级别是：

存储等级 | 说明
--- | ---
MEMORY_ONLY | 将 RDD 作为反序列化的 Java 对象存储到 JVM 中。如果内存用尽，那么某些分区将不会被缓存，每次需要时都会重新计算，这是默认级别。
MEMORY_AND_DISK | 将 RDD 作为反序列化的 Java 对象存储到 JVM 中。 如果内存用尽，多余的分区会存储到磁盘，并在需要时从磁盘读取。
MEMORY_ONLY_SER | 将 RDD 作为序列化的 Java 对象（每个分区有一个字节数组）进行存储。这通常比反序列化对象更节省空间，特别是在使用快速序列化器（Kryo）时，但 CPU 读取更频繁。
MEMORY_AND_DISK_SER | 和 MEMORY_ONLY_SER 相似，内存用尽后多余的分区会存储到磁盘中，在需要时避免重新计算这部分分区。
DISK_ONLY | 仅存储 RDD 分区到磁盘中。 
MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. | 和上面的等级类似，但是会复制每一个分区到两个集群结点中。
OFF_HEAP (experimental) | 和 MEMORY_ONLY_SER 类似, 但是在堆外内存中存储数据，这需要启用堆外内存。

**注意：**在 Python 中，存储的对象将始终使用 Pickle 库进行序列化，因此您是否选择序列化级别并不重要。Python中的可用存储级别包括MEMORY_ONLY，MEMORY_ONLY_2，MEMORY_AND_DISK，MEMORY_AND_DISK_2，DISK_ONLY和DISK_ONLY_2。

即使没有用户调用持久化操作，Spark 也会在 shuffle 操作（例如 reduceByKey）中自动保留一些中间数据。 这样做是为了避免在 shuffle 期间节点发生故障时重新计算整个输入。如果计划重用 RDD，我们仍然建议用户调用持久化操作。

**选择哪个存储级别？**
Spark 的存储级别意味着提供了在内存使用和 CPU 利用率上的权衡选择。我们建议通过以下流程进行选择：

如果你的 RDD 能够适应默认的存储级别（MEMORY_ONLY），那么请保持这种状态。这是利用 CPU 效率最高的选项，使得在 RDD 上的运算尽可能快的运行。

如果不能适应默认的存储级别，请使用 MEMORY_ONLY_SER 选项，并且选择一个高效的序列化库，使得序列化的对象不仅能够节省空间，还能比较快速的访问。（适用于 Java 和 Scala）

除非 RDD 的计算函数特别昂贵或者需要过滤掉大量的数据，否则不要把数据溢出到磁盘上。另外，重新计算分区可能和直接从磁盘读取结果一样快。

如果你想要更快的容错恢复（例如，使用 Spark 去处理来自 Web 应用的请求），请使用复制存储等级。所有存储级别通过重新计算丢失的数据提供完全容错，但复制的存储级别允许您继续在 RDD 上运行任务，而无需等待重新计算丢失的分区。

**删除数据**
Spark 会自动监视每个节点上的缓存使用情况，并以最近最少使用（LRU）的方式删除旧数据分区。如果您想手动删除RDD而不是等待它自己退出缓存，请使用 RDD.unpersist() 方法。

### 6. 共享变量
通常，当用户定义的函数传递给远程集群结点上执行的 Spark 运算（例如 map 和 reduce）时，函数中使用的变量都会有单独的副本，这些变量被复制到每台机器上，并且变量在远程计算机上的更新不会传回 driver 程序。普遍支持跨 tasks 的读写共享变量是低效的。但是，Spark 为常见的使用模式提供了两种有限类型的共享变量：广播变量和累加器。

**广播变量(Broadcast Variables)**
广播变量允许程序在每台机器上缓存一个只读变量，而不是发送变量副本到每个 task 中。例如，这是一个有效的方式为每个节点提供较大集合的输入副本。Spark 通过使用高效的广播算法来分发广播变量，来减少通信的代价。

Spark 通过一系列的 stage 来执行 action，stage 通过 “shuffle” 操作来分割。Spark 会自动广播每一个 stage 中 tasks 需要的通用数据，广播的数据会以序列化的形式缓存起来，并在运行每一个任务之前做反序列化。这意味着显式创建广播变量仅在跨多个阶段的任务需要相同数据，或想要以反序列化形式缓存数据时才有用。

广播变量通过向 SparkContext.broadcast() 方法传入变量 v 来创建 v 的广播变量，该方法返回的是一个变量 v 的包装器，可以通过调用 value 方法访问具体的值。可参考如下的代码：

```scala
scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)

scala> broadcastVar.value
res0: Array[Int] = Array(1, 2, 3)
```
创建广播变量后，应该在群集上运行的任何函数中使用它而不是变量 v，这样变量 v 不会多次传送到节点。另外，在广播之后不应修改变量 v，以便确保所有节点获得广播变量的相同值（例如，如果稍后将变量发送到新节点）。

**累加器(Accumulators)**

累加器是一个只能做累加操作的变量，通过交换和结合来有效的支持并行化处理。累加器可以用来实现计数器或者求和运算，Spark 本身支持数值类型的累加器，程序员可以根据需要添加新类型的支持。

作为用户，你可以给累加器指定名称，如下图所示，一个命名的累加器（名称叫做 counter）将显示在 Web UI 中。Spark 会在 'Tasks' 列表中显示每个 task 中累加器的值。

![](/assets/images/201902/15508297683697.jpg)

跟踪UI中的累加器对于理解运行 stages 的进度非常有用（注意：Python中尚不支持）。

数值类型的累加器可以通过调用 SparkContext.longAccumulator() 或者 SparkContext.doubleAccumulator() 来创建，运行在集群中的 tasks 通过调用 add 方法进行累加操作。然而，在 Executor 端不能读取累加器的值，只能在 Driver 端使用 value 方法读取累加器的值。

下面的代码展示了累加器被用来统计数组的和：
```scala
scala> val accum = sc.longAccumulator("My Accumulator")
accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)

scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))
...
10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

scala> accum.value
res2: Long = 10
```
虽然此代码使用了内置支持的 Long 类型累加器，但程序员也可以通过继承 AccumulatorV2 来创建自己需要的类型的累加器。 AccumulatorV2 抽象类有几个必须 override 方法：reset 方法用于将累加器重置为零，add 方法用来累加一个值，merge 操作用于和另一个相同类型的累加器进行合并，其他必须覆盖的方法包含在 API 文档中。这里举一个例子，假设我们有一个表示数学向量的 MyVector 类，我们可以写：

```scala
class VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] {

  private val myVector: MyVector = MyVector.createZeroVector

  def reset(): Unit = {
    myVector.reset()
  }

  def add(v: MyVector): Unit = {
    myVector.add(v)
  }
  ...
}

// Then, create an Accumulator of this type:
val myVectorAcc = new VectorAccumulatorV2
// Then, register it into spark context:
sc.register(myVectorAcc, "MyVectorAcc1")
```

请注意，当程序员定义自己的 AccumulatorV2 类型时，结果类型可能与添加的元素类型不同。

对于仅在 actions 内执行的累加器更新，Spark 保证每个 task 对累加器的更新仅作用一次，即重新启动的 task 不会重复更新该值。在 transformations 中，如果 task 或某个 Job 的 stage 重新执行，则可能出现重复的更新操作。

累加器也会受 Spark 的惰性模型影响。如果在 RDD 的运算中进行累加操作，则只有在执行到 action 操作时才会更新它们的值。因此，在像 map() 这样的惰性 transformation 中进行累积器更新时，不能保证已经执行了累加器更新。以下代码片段演示了此属性：
```scala
val accum = sc.longAccumulator
data.map { x => accum.add(x); x }
// Here, accum is still 0 because no actions have caused the map operation to be computed.
```

### 7. 部署集群
[程序提交指南](http://spark.apache.org/docs/latest/submitting-applications.html) 介绍了如何将应用程序提交到集群。总之，一旦将应用程序打包到 JAR（使用Java/Scala）或一组 .py 或 .zip 文件（使用 Python），bin/spark-submit 脚本允许你提交给所有的集群管理器。

### 8. 从 Java/Scala 程序中启动 Spark 作业
[org.apache.spark.launcher](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/launcher/package-summary.html) 包提供了使用 Java API 将 Spark 作业作为子进程启动的类。

### 9. 单元测试
Spark 能够比较好的支持现在流行的单元测试框架。只需在测试中创建一个 SparkContext，master 的值设置为 local，运行你的操作，然后调用 SparkContext.stop() 将其停止。确保在 finally 块或测试框架的 tearDown 方法中停止上下文，因为 Spark 不支持在同一程序中同时运行的两个上下文。

### 10.下一步怎么办？
你可以看一些 [Spark 程序例子](https://spark.apache.org/examples.html) 在该页面中。另外，Spark 源码中包括几个样例在 example 目录([Scala](https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples)、[Java](https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples)、[Python](https://github.com/apache/spark/tree/master/examples/src/main/python)、[R](https://github.com/apache/spark/tree/master/examples/src/main/r))下。你可以把类名作为参数传递给 bin/run-example 脚本，来运行 Java 或 Scala 的样例。例如：
```bash
./bin/run-example SparkPi
```
对于 Python 的样例，用 spark-submit 来替代:
```bash
./bin/spark-submit examples/src/main/python/pi.py
```
对于 R 的样例，用 spark-submit 来替代:
```bash
./bin/spark-submit examples/src/main/r/dataframe.R
```
[配置](http://spark.apache.org/docs/latest/configuration.html) 和 [调优指南](http://spark.apache.org/docs/latest/tuning.html) 提供了关于优化程序的最佳实践的信息。确保在内存中以高效的格式存储数据尤为重要。有关部署的帮助，[集群模式概述](http://spark.apache.org/docs/latest/cluster-overview.html) 描述了分布式操作和支持的集群管理器中涉及的组件。

最后，[Scala](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.package)、[Java](http://spark.apache.org/docs/latest/api/java/)、[Python](http://spark.apache.org/docs/latest/api/python/) 和 [R](http://spark.apache.org/docs/latest/api/R/) 中提供了完整的 API 文档。
